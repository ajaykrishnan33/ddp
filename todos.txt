IRGAN model will accept the recipe steps (text context)  + images (with one of the choices replacing the empty placeholder) as input 

1. Modify the "Visual Cloze" dataset as follows:
	a. Every question to be modified so that it is always trying to predict the last of a sequence of images.
	b. The choice_list for every question to be expanded to around 20 (configurable) images.
	c. The choice_list to be shuffled so that the corerct "answer" value is uniformly in the range [0, MAX_CHOICES).

2. Create vocabulary file from context words. Remove special chars? 


3. Doc2Vec for paragraph encoding
	- Train doc2vec model separately on sentences from the training data : pre-training
		- Also need to repeat this for the validation data, but separately from the training data. This is okay since this pre-training is unsupervised anyway. These embedding vectors will only be used 
	- Currently, all periods have been stripped. All contexts form one long meaningless sentence. Should I add the period back? Then will have to modify the doc2vec model to stop picking n-grams at each period.
	- The output will finally be a 100-d vector for every document.	This can be used as the initial weights for the embedding layer in the actual model. 
	- Will have to write code to read the generated docvectors and initialize embedding weights. The generator and discriminator models will do this.
	- Problem: The embedding weights cannot be trained any further since they will not include documents present only in the validation set. Rather, allowing the embedding weights to be trained will only result in the training loss reducing but at validation time, since the embedding weights will be completely swapped out with the document vectors for the validation docs, the additional training will be completely useless here and is likely to give poor results.

4. Word2Vec + RNN for paragraph encoding
	- First problem: total of more than 9000 words in the vocabulary. This was after reducing the vocab size from 42000 - around 33000 words had frequency of less than 10. These were discarded.
	- Second problem: some context bodies have more than 2000 words. Need to get exact statistics on this. This means that the vector sizes for the context bodies have to be large. Not sure how much of a problem this really is.
	- Due to the large number of words per context body, will make sense to use LSTMs or GRUs for encoding the words.

5. Image encoding
	- VGGNet/AlexNet
		- Transform all images to size required by the chosen model.
		- The model can be initialized with pre-trained weights and trained further.

6. Generator
	- Will first
		- encode the context vectors using doc2vec/(word2vec+RNN) embeddings.
		- encode the question images using AlexNet/VGGNet
			- encode each image into a 4096 dimension vector using VGG16
			- Two choices from here:
				- Use an RNN to encode the set of vgg encodings of the images
					- Advantage: No assumptions about number of images in each data item
					- Disadvantage: Loss of information
				- Concatenate the vgg encodings of the images
					- Advantage: Full info preserved
					- Disadvantage: Assumption about number of images in each data item
			- Going to choose option 1 here
		- pass the encoding of the context vectors to an RNN and the question images to another RNN and concatenate the two outputs to form the doc encoding.
	- Next, for a given image choice, it will:
		- encode the image choice using the same model as above.
		- compute the cosine similarity between the doc encoding and the choice encoding as a single numeric score.
		- alternately, the cosine similarity function can be replaced by a weighted model that outputs a single numeric score
	- The numeric score will represent the likelihood of the choice being the correct answer to the visual cloze question.

	Pre-training the Generator,
		- We will ultimately get a whole of vector of numeric scores - one for every choice. (This can be computed in parallel?).
		- Then apply a softmax on this vector to get a probability distribution over the choice list.
		- This probability distribution is compared against the one-hot vector for the true answer to compute the loss for the generator model.

7. Discriminator
	- Will first
		- encode the context vectors using doc2vec/(word2vec+RNN) embeddings.
		- encode the question images using AlexNet/VGGNet
		- pass the encoding of the context vectors to an RNN and the question images to another RNN and concatenate the two outputs to form the doc encoding.
	- Next, for a given image choice, it will:
		- encode the image choice using the same model as above.
		- compute the cosine similarity between the doc encoding and the choice encoding as a single numeric score.
		- alternately, the cosine similarity function can be replaced by a weighted model that outputs a single numeric score
	- The numeric score will represent the likelihood of the choice being the correct answer to the visual cloze question.

	Pre-training the Discriminator,
		- Apply the sigmoid function to the output numeric score to get a value in the [0-1] range. This will make it look like a probability score.
		- For every image choice, we have a 0-1 label indicating whether it really is the correct answer. Use this to compute the loss for the discriminator model.

8. Adversarial Training
	For E epochs:
		For g steps:
			- First the Discriminator is assumed to be fixed and the Generator is trained.
			- The Generator will take in an input doc - consisting of a list of context bodies, question images and choice images as input and generate a probability distribution of relevance over the choice list.
			- A set of K images will be sampled from the choice list using this probability distribution, and passed through the discriminator along with the input doc. The Generator's image choices should be such that the Discriminator is fooled into thinking that the images are indeed relevant and hence must have an output label of 1. If the Discriminator is not fooled, then the Generator is penalized and must update its weights.
		For d steps:
			- Now, the Generator is assumed to be fixed and the Discriminator is trained.
			- The Generator is used to generate N sample images for the input doc. These images must be classified by the Discriminator as irrelevant. These images are combined with the positive sample from the input doc and passed through the Discriminator.
			- The Discriminator accrues a loss for every misclassified image and its weights are updated using this loss.
		Validation loss is computed here.



Idea:

1. Joint encoding of question images and contexts:
	- Instead of separately encoding the images and the text and then concatenating them together, what if we train a model that takes as input an image and its corresponding description and then 
		- passes the description through an LSTM/GRU/RNN/Doc2Vec to obtain a condensed representation of size X
		- passes the image through VGGNet followed by some number of linear layers to obtain a condensed representation of size X.
		- tries to maximize the cosine similarity between the two representations.
	- This will be an unsupervised learning model that will try to come up with two networks that can encode text and images into the same vector space (semantically).
	- Might not make sense to use Doc2Vec here since it is of no use to train it further
	- Worry: what if the model always outputs 0 as the vector? What is the guarantee that the common encoding has any semantic information from the image or the text?
	- Possible solution: 
		- Assume there exists a lare dataset that contains images and their very detailed descriptions, such that whatever semantic information is available in the image is also available in the descriptions.
		- Now, we need some additional metadata to be available on these images - eg. QA pairs or class labels which can be used as an image task for verifying that the common encoding has semantic information.
		- 3 networks are defined:
			- Network 1: Takes an image as input to obtain a vector of size X.
			- Network 2: Takes the description as input to obtain a vector of size X.
			- Network 3: This network represents some image task like QA or classification, that accepts as input a vector of size X.
		- Now, pass the image through N1 and get the vector V1. 
		- Simultaneously, pass the image through N2 and get the vector V2.
		- Now, pass the vector V1 through N3 and V2 through N3 independently and compute losses two losses: L31 and L32 for the network N3.
		- Now also compute the similarity between V1 and V2 = L12 (where lower value of L12 means greater similarity).
		- Total Loss, L = L12 + L31 + L32. Train the 3 networks to minimize this loss.
		- Ultimately, the networks N1 and N2 must create very similar vectors V1 and V2 for an unseen image and its corresponding description and this vector, when passed through N3 must produce the correct answer. This will indicate that the networks N1 and N2 have learnt to extract the semantic information from the image and the text and encode them in a common domain.
		- Finally, the trained networks N1 and N2 can be used as encoding networks for images and text respectively.
	- Possible problem:
		- It is unlikely that there exists a dataset with image descriptions that almost completely captures the semantic information present in an image.
		- Even if there exists such a dataset, why can't we simply consider the encoding of the description to be the common image-text encoding? (This is assuming that the image descriptions were generated by some network in the first place. If not, then we obviously won't have these encodings.)
		- If we had to make such a dataset ourselves, how would we do this?
			- Identify different levels of semantic information:
				- Object detection and identification
				- Object sizes, colors, shapes
				- Relative positions of objects
				- Interaction of objects with each other (Action detection and identification)
			- If we were to even identify all these types of semantic information, how will we ensure a common representation for them in the first place?
	- Is there an adversarial solution close by to this?




