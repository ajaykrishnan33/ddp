IRGAN model will accept the recipe steps (text context)  + images (with one of the choices replacing the empty placeholder) as input 

1. Modify the "Visual Cloze" dataset as follows:
	a. Every question to be modified so that it is always trying to predict the last of a sequence of images.
	b. The choice_list for every question to be expanded to around 20 (configurable) images.
	c. The choice_list to be shuffled so that the corerct "answer" value is uniformly in the range [0, MAX_CHOICES).

2. Create vocabulary file from context words. Remove special chars? 


3. Doc2Vec for paragraph encoding
	- Train doc2vec model separately on sentences from the training data : pre-training
		- Also need to repeat this for the validation data, but separately from the training data. This is okay since this pre-training is unsupervised anyway. These embedding vectors will only be used 
	- Currently, all periods have been stripped. All contexts form one long meaningless sentence. Should I add the period back? Then will have to modify the doc2vec model to stop picking n-grams at each period.
	- The output will finally be a 100-d vector for every document.	This can be used as the initial weights for the embedding layer in the actual model. 
	- Will have to write code to read the generated docvectors and initialize embedding weights. The generator and discriminator models will do this.
	- Problem: The embedding weights cannot be trained any further since they will not include documents present only in the validation set. Rather, allowing the embedding weights to be trained will only result in the training loss reducing but at validation time, since the embedding weights will be completely swapped out with the document vectors for the validation docs, the additional training will be completely useless here and is likely to give poor results.

4. Word2Vec + RNN for paragraph encoding
	- First problem: total of more than 9000 words in the vocabulary. This was after reducing the vocab size from 42000 - around 33000 words had frequency of less than 10. These were discarded.
	- Second problem: some context bodies have more than 2000 words. Need to get exact statistics on this. This means that the vector sizes for the context bodies have to be large. Not sure how much of a problem this really is.
	- Due to the large number of words per context body, will make sense to use LSTMs or GRUs for encoding the words.

5. Image encoding
	- VGGNet/AlexNet
		- Transform all images to size required by the chosen model.
		- The model can be initialized with pre-trained weights and trained further.

6. Generator
	- Will first
		- encode the context vectors using doc2vec/(word2vec+RNN) embeddings.
		- encode the question images using AlexNet/VGGNet
			- encode each image into a 4096 dimension vector using VGG16
			- Two choices from here:
				- Use an RNN to encode the set of vgg encodings of the images
					- Advantage: No assumptions about number of images in each data item
					- Disadvantage: Loss of information
				- Concatenate the vgg encodings of the images
					- Advantage: Full info preserved
					- Disadvantage: Assumption about number of images in each data item
			- Going to choose option 1 here
		- pass the encoding of the context vectors to an RNN and the question images to another RNN and concatenate the two outputs to form the doc encoding.
	- Next, for a given image choice, it will:
		- encode the image choice using the same model as above.
		- compute the cosine similarity between the doc encoding and the choice encoding as a single numeric score.
		- alternately, the cosine similarity function can be replaced by a weighted model that outputs a single numeric score
	- The numeric score will represent the likelihood of the choice being the correct answer to the visual cloze question.

	Pre-training the Generator,
		- We will ultimately get a whole of vector of numeric scores - one for every choice. (This can be computed in parallel?).
		- Then apply a softmax on this vector to get a probability distribution over the choice list.
		- This probability distribution is compared against the one-hot vector for the true answer to compute the loss for the generator model.

7. Discriminator
	- Will first
		- encode the context vectors using doc2vec/(word2vec+RNN) embeddings.
		- encode the question images using AlexNet/VGGNet
		- pass the encoding of the context vectors to an RNN and the question images to another RNN and concatenate the two outputs to form the doc encoding.
	- Next, for a given image choice, it will:
		- encode the image choice using the same model as above.
		- compute the cosine similarity between the doc encoding and the choice encoding as a single numeric score.
		- alternately, the cosine similarity function can be replaced by a weighted model that outputs a single numeric score
	- The numeric score will represent the likelihood of the choice being the correct answer to the visual cloze question.

	Pre-training the Discriminator,
		- Apply the sigmoid function to the output numeric score to get a value in the [0-1] range. This will make it look like a probability score.
		- For every image choice, we have a 0-1 label indicating whether it really is the correct answer. Use this to compute the loss for the discriminator model.


8. Adversarial Training
	For E epochs:
		For g steps:
			- First the Discriminator is assumed to be fixed and the Generator is trained.
			- The Generator will take in an input doc - consisting of a list of context bodies, question images and choice images as input and generate a probability distribution of relevance over the choice list.
			- A set of K images will be sampled from the choice list using this probability distribution, and passed through the discriminator along with the input doc. The Generator's image choices should be such that the Discriminator is fooled into thinking that the images are indeed relevant and hence must have an output label of 1. If the Discriminator is not fooled, then the Generator is penalized and must update its weights.
		For d steps:
			- Now, the Generator is assumed to be fixed and the Discriminator is trained.
			- The Generator is used to generate N sample images for the input doc. These images must be classified by the Discriminator as irrelevant. These images are combined with the positive sample from the input doc and passed through the Discriminator.
			- The Discriminator accrues a loss for every misclassified image and its weights are updated using this loss.
		Validation loss is computed here.

Update: 16th February
1. Had to pre-extract features using VGG since loading the whole network at once was severely restricting the size of the batches in the training process due to memory limitations. The disadvantage of this solution is that it won't be possible to continue training the VGG params for this task.
2. Faced another problem with pre-training the discriminator: the classes are unbalanced so the model is simply giving 0 as the answer independent of the input. 
	One possible solution is to randomly over-sample the positive samples so that the two classes are almost balanced.
	Another solution is to take pairs of samples - one positive and the other negative (randomly chosen) and get the model to predict which one is more relevant - maximize the difference between their cosine similarities with the encoding of the question. (I think IRGAN uses this.)
		This solution will require changes to be made to the data loading process, the discriminator and the generator as well. 
		For every question, generate pairs (A, B) from the choice list, where A is the correct answer and B is a wrong answer. In total, for every question, there will be 19 such pairs.
		Every dataset entry will consist of a question (consisting of the question images and the context) and an answer pair of the form (A,B).
		The discriminator takes as input one dataset entry and computes the cosine similarity of the question encoding with each answer encoding. The difference between the cosine similarity of values of A and B is computed and maximized by the training process.
		The generator takes as input one dataset entry, takes the less relevant image B and generates a probability distribution over the choice list of an image being more relevant to the question than B - basically trying to mimic (A, B).


Idea:

1. Joint encoding of question images and contexts:
	- Instead of separately encoding the images and the text and then concatenating them together, what if we train a model that takes as input an image and its corresponding description and then 
		- passes the description through an LSTM/GRU/RNN/Doc2Vec to obtain a condensed representation of size X
		- passes the image through VGGNet followed by some number of linear layers to obtain a condensed representation of size X.
		- tries to maximize the cosine similarity between the two representations.
	- This will be an unsupervised learning model that will try to come up with two networks that can encode text and images into the same vector space (semantically).
	- Might not make sense to use Doc2Vec here since it is of no use to train it further
	- Worry: what if the model always outputs 0 as the vector? What is the guarantee that the common encoding has any semantic information from the image or the text?
	- Possible solution: 
		- Assume there exists a lare dataset that contains images and their very detailed descriptions, such that whatever semantic information is available in the image is also available in the descriptions.
		- Now, we need some additional metadata to be available on these images - eg. QA pairs or class labels which can be used as an image task for verifying that the common encoding has semantic information.
		- 3 networks are defined:
			- Network 1: Takes an image as input to obtain a vector of size X.
			- Network 2: Takes the description as input to obtain a vector of size X.
			- Network 3: This network represents some image task like QA or classification, that accepts as input a vector of size X.
		- Now, pass the image through N1 and get the vector V1. 
		- Simultaneously, pass the image through N2 and get the vector V2.
		- Now, pass the vector V1 through N3 and V2 through N3 independently and compute losses two losses: L31 and L32 for the network N3.
		- Now also compute the similarity between V1 and V2 = L12 (where lower value of L12 means greater similarity).
		- Total Loss, L = L12 + L31 + L32. Train the 3 networks to minimize this loss.
		- Ultimately, the networks N1 and N2 must create very similar vectors V1 and V2 for an unseen image and its corresponding description and this vector, when passed through N3 must produce the correct answer. This will indicate that the networks N1 and N2 have learnt to extract the semantic information from the image and the text and encode them in a common domain.
		- Finally, the trained networks N1 and N2 can be used as encoding networks for images and text respectively.
	- Possible problem:
		- It is unlikely that there exists a dataset with image descriptions that almost completely captures the semantic information present in an image.
		- Even if there exists such a dataset, why can't we simply consider the encoding of the description to be the common image-text encoding? (This is assuming that the image descriptions were generated by some network in the first place. If not, then we obviously won't have these encodings.)
		- If we had to make such a dataset ourselves, how would we do this?
			- Identify different levels of semantic information:
				- Object detection and identification
				- Object sizes, colors, shapes
				- Relative positions of objects
				- Interaction of objects with each other (Action detection and identification)
			- If we were to even identify all these types of semantic information, how will we ensure a common representation for them in the first place?
	- Is there an adversarial solution close by to this?

02-03-2019

An Image consists of a very low-level representation of information whereas the caption for the image consists of a very high level representation of the same (or subset of the) information. In general, an image will always capture much more information than its caption - implying that any encoding of an image must capture more information than an equivalent encoding of its caption, if both the encodings lie in a common vector space, i.e., the information of the caption encoding must be contained within the information of the image encoding. Assume that we have a network G1 that is a variational autoencoder (VAE) for images. Pass an image I through this network and obtain an encoding G1(I). Similarly, we have one more network G2 which is also a variational autoencoder for the caption. The caption C_I of the image I is passed through G2 to obtain the encoding G2(C_I). We will build one more network D, that accepts as input an image D and an encoding E as input and determines whether the encoding E contains some of the information present in the image I. The networks G1 and G2 are trained to convince the discriminator into thinking that the encodings G1(I) and G2(C_I) both contain information of the image I. 

Problems:
1. How will ever pre-train D, since the encoding E can only be obtained from the networks G1 and G2 and is not naturally available?
2. What will be the negative samples for D? All samples generated from G1 and G2 and passed to D will be positive samples. Should we artificially construct negative samples by pairing an image with an irrelevant encoding?
	- Alternatively, every training sample for D can consist of < I, E1, E2 > where E1 is the correct encoding and E2 is incorrect and the loss function can maximize the difference between the cosine similarities of encoding(I) with E1 and E2. 
	- Won't the encoder part of D (for I) be identical to the encoder part of G1? Won't it just learn to mimic the encoder part of G1
3. In this scheme, if D only takes an encoding (without the image) and tries to predict whether it came from the image distribution or from the caption distribution, would that also be equivalent?
4. This feels like a simplification of CM-GANs (https://dl.acm.org/citation.cfm?id=3284750).



16-04-2019

1. Need to revive this.
2. Modify model to use Google pre-trained Word2Vec embeddings.
3. LSTM to encode the context.
4. Implement evaluation metric.
5. Find baseline from paper (no need to re-implement)


Order:
data_prepare.py
verify_data.py
create_vocab.py
cleanup_vocab.py